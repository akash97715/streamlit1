
				LEARNING GUIDE TO YOUR MODEL


•fill_na function
Removing or filling the NaN and undefined values in the dataset is an important step in Exploratory Data Analysis.
You can either remove the particular row or column containing NaN values or else fill those values with the mean,median etc. depending on the usecase.

Importing the essential tools
We use tools provided by pandas to perform this function. So all we have to import is pandas library.
import pandas

Function definition
def fill_na(dataframe):

    for col in dataframe.columns:
        if dataframe[col].dtype.name != 'object':
            if (dataframe[col].isnull().sum())*2 >= num_rec:
                dataframe = dataframe.drop([col], axis=1)
            else:
                dataframe[col] = dataframe[col].fillna(dataframe[col].mean())
    return dataframe

Function call
Input your dataset as the parameter to this function, to handle the NaN values in your dataset.
fill_na(dataframe)

Check out the documentation here : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html






•scale function
Your dataset may contain a wide variety of values. Inorder to process your data further it is essential to scale down your data in the desired range based on the usecase. 
This function helps you to do so.

Importing the essential tools
from sklearn.preprocessing import RobustScaler,StandardScaler

Function definition
def scale(X_train,X_test):
    sc = StandardScaler()   
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    return X_train, X_test

Function call
The X_train and X_test are passed in as the parameters while calling the function.
scale(X_train,X_test)

For more info visit : https://benalexkeen.com/feature-scaling-with-scikit-learn/






•splitdata function
Splits your dataset into train data for training the model and test data. This functionality is provided by the scikit-learn library.

Importing the essential tools
from sklearn.model_selection import train_test_split

Function definition
def splitdata(X,y):
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)
    return X_train, X_test, y_train, y_test


Function call
Pass in your independent and dependent variables into the function while calling and that's it you get a train test split of your dataset.
splitdata(X,y)


Check out more : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html






•encode function
This function converts the categorical features in your dataset and convert it into numerical values.
One Hot encoding produces three columns and the presence of a class is represented in binary format. The three classes are separated out to three different features.
Label encoding or Ordinal encoding gives numerical aliases to the classes. Numerical labels are always between 1 and the number of classes.

Importing the essential tools
from sklearn.preprocessing import LabelEncoder

Function definition

>>>def encode(dataframe):
...    
...    for col in dataframe.columns:
           if dataframe[col].dtype.name == 'object':
               le = LabelEncoder()
               dataframe[col] = le.fit_transform(dataframe[col])
       return dataframe

Function call
Your dataset is passed in as the parameter for this function.
encode(dataframe)

Learn more : https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/






•oversample function
This function converts an imbalanced datset to a balanced one by oversampling. There are 2 ways of balancing a dataset. They are;
Random Oversampling:- where examples in the minority class are randomly dupliated.
Random Undersampling:-where examples in the majority class are randomly deleted.

Importing the essential tools
from imblearn.over_sampling import SMOTE,RandomOverSampler

Function definition
def oversample(X,y):
    smote = random.choice([SMOTE(),RandomOverSampler()])
    X,y = smote.fit_resample(X,y)
    return X,y

Function call
oversample(X,y)
The independent and dependent features are given in as the parameters. SMOTE stands for Synthetic Minority Oversampling Technique. 
This is a statistical technique for increasing the number of cases in your dataset in a balanced way.

Check out more about balancing datasets : https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/
Learn more about smote : https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis




•dupli function
It detects the duplicate rows in your dataset and removes them. The different ways of handling the datasets using this function can be found out using the links provided. 
Importing the essential tools
It is actually imported from pandas and all you need to import is pandas for this function
import pandas

Function definition
def dupli(dataframe):
    dataframe.drop_duplicates(subset=None, keep='first', inplace=True)
    return dataframe

Function call
Pass in the dataframe as the parameter for this function and it will remove the duplicate rows in your dataset.
dupli(dataframe)

Check out the documentation : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html



• Adaboost classifier function
Ada-boost, like Random Forest Classifier is another ensemble classifier.
Ada-boost classifier combines weak classifier algorithm to form strong classifier.
A single algorithm may classify the objects poorly. But if we combine multiple classifiers with selection of training set at every iteration and assigning right amount of weight in final voting, 
we can have good accuracy score for overall classifier.

Importing the essential tools
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier


Function definition
def adaboostclassifier(X_train,X_test,y_train,y_test):

    classifier = AdaBoostClassifier()
    clffit = classifier.fit(X_train,y_train)
    parameters = [{'base_estimator':[None]}]
    gs = GridSearchCV(estimator = clffit,
                      param_grid = parameters,
                      n_jobs = -1,
                      scoring = 'accuracy',
                      cv = 2)
    gs.fit(X_train, y_train)
    classifier = gs.best_estimator_
    classifier.fit(X_train,y_train)
    return classifier.predict(X_test), gs.best_params_

Function call
adaboostclassifier(X_train,X_test,y_train,y_test)

Check out the documentation here : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html
Learn more : https://en.wikipedia.org/wiki/AdaBoost


